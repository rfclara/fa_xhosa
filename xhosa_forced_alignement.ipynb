{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rfclara/fa_xhosa/blob/main/xhosa_forced_alignement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aligning transcriptions and annotations - Xhosa corpus\n",
        "\n"
      ],
      "metadata": {
        "id": "FffJ6hD7dVqw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jKa71Y6_VYK"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pourpose of this notebook is to align the interlinear glosses with the audio, from a transcribed corpus in Xhosa, one of the official languages of South Africa and Zimbabwe.\n",
        "\n",
        "The transcription of this corpus are not aligned with the speech. We will use [CCT forced alignement](https://pytorch.org/audio/main/tutorials/ctc_forced_alignment_api_tutorial.html) in order to cut the recording into small chunks and get the timestamps corresponding to their transcriptions.\n",
        "\n",
        "\n",
        "Here, we will follow the necessary steps to prepare the data for training or fine-tuning a speech-to-text model,"
      ],
      "metadata": {
        "id": "GfrP1cKUDhAo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHqkn2_OCz4Y"
      },
      "source": [
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/cawoylel/nlp4all/blob/main/asr/illustrations/forced_aligner.png?raw=true:, width=200\" alt=\"transformer\" width=500 class=\"center\">\n",
        "<br>\n",
        "    <em>\n",
        "    Illustration of the task of Forced Alignement, from nlp4all\n",
        "    </em>\n",
        "</p>\n",
        "\n",
        "[MMS](https://github.com/facebookresearch/fairseq/blob/main/examples/mms/README.md) is a Forced Aligner using a multilingual speech model trained on more than one thousand languages. You can check here if your language is included: https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZM5ZQKY_SEJ"
      },
      "source": [
        "# Installing the dependencies in the virtual environnement\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libicu-dev pkg-config"
      ],
      "metadata": {
        "id": "SwpHPRFVs032"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddzlqgyTNrVB"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install libsox-fmt-all sox ffmpeg # needed for processing audio\n",
        "!apt install libicu-dev pkg-config # needed for processing text and unicode symbols"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libicu-dev pkg-config\n",
        "!pip install -q ICU-Tokenizer"
      ],
      "metadata": {
        "id": "aR-bQ5cBtSH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O-nLC2sN3UC"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torch torchaudio -y # we need to install the nightly version of torch\n",
        "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
        "!pip install -q dataclasses\n",
        "!pip install -q sox # for audio processing\n",
        "!pip install -q ICU-Tokenizer # for tokenizing the text\n",
        "!pip install -q datasets # we will use huggingface datasets for loading the training dataset\n",
        "!pip install pandas\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2oC8fZIA6MI"
      },
      "source": [
        "In this step, we clone repositories containing code and resources essential for our ASR project. Specifically, we clone the `rfclara/fa_xhosa` repository, and the `isi-nlp/uroman` repository, which provides functionalities for Romanization of text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty-GLgwQ_VOn"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/isi-nlp/uroman.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYVw62egXVtj"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "git clone https://github.com/facebookresearch/fairseq.git\n",
        "cd fairseq\n",
        "pip install --editable ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rfclara/fa_xhosa"
      ],
      "metadata": {
        "id": "9gAz4VqNqVGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepearing the data\n",
        "Getting the audio files and the transcriptions.\n",
        "\n",
        "`original.zip` should decompress into one folder called `original` containing the audio files and the transcirptions. Each filename must match and differe only by its extension (.wav, .xlsx)\n",
        "\n",
        "example :\n",
        "story1.wav\n",
        "story1.xlsx"
      ],
      "metadata": {
        "id": "wqNEaVzM2hAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EITHER** give this notebook acces to your drive:\n",
        "run next cell. It will ask for the permission to acces your drive and it will copy the archive from your Drive to the virtual environnement.\n",
        "\n",
        "Make sure `xhosa.zip` is placed directly in the main directory of your Drive. If not, you may change the path on the cell.\n",
        "\n",
        "example :\n",
        "`!cp /content/drive/MyDrive/your/actual/path/original.zip /content`"
      ],
      "metadata": {
        "id": "Fj3aZfAmmRm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS CELL IS OPTIONAL\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/original.zip /content"
      ],
      "metadata": {
        "id": "MoS9NCsm6Zje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__OR__ upload xhosa.zip directly here (Files > upload)."
      ],
      "metadata": {
        "id": "ewxzASa6MDSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/xhosa\n",
        "!unzip /content/original.zip -d /content/xhosa"
      ],
      "metadata": {
        "id": "KXE-mpeULtBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QgFtPtIuKlL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting the .xlsx transcriptions to .txt"
      ],
      "metadata": {
        "id": "i8MMjTcoJnV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/fa_xhosa/extract_transcriptions.py /content/xhosa/original /content/xhosa/ready"
      ],
      "metadata": {
        "id": "RCgQ7jT6_AJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell will display the first 10 lines of the first trancription."
      ],
      "metadata": {
        "id": "LId9xxyqyzrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 $(ls /content/xhosa/ready/*.txt | head -n 1)"
      ],
      "metadata": {
        "id": "H50h3opOyyuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXa7lmZ-CV0s"
      },
      "source": [
        "### Resampling the audios  <a name=\"resampling\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyZtr8YXCaBR"
      },
      "source": [
        "After downloading the audios, we need to resample them. Many modern speech models only deal with *16 000 sampling*. We will use `ffmpeg` to resample the audios into 16 000. We will also save the resampled audios into `.wav` files."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will save the resampled wav files into a new `ready` directory containing all the extracted transcriptions .txt."
      ],
      "metadata": {
        "id": "rdUS_kHtSR2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQOR_M2AQqDQ"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "for f in /content/xhosa/original/*.wav; do\n",
        "  filename=\"$(basename \"$f\")\"\n",
        "  directory=\"$(dirname \"$f\")\"\n",
        "  stem=${filename%.*}\n",
        "  ffmpeg -i \"$f\" -ac 1 -ar 16000 \"/content/xhosa/ready/${stem}.wav\" ;\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHPQxYv3M9oq"
      },
      "source": [
        "## Neural Forced Alignment  <a name=\"aligner\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using [torchaudio.functional.forced_align()](https://pytorch.org/audio/stable/generated/torchaudio.functional.forced_align.html#torchaudio-functional-forced-align) the following cell will automatically align each line of the transcription with its corresponding time in the audio file. This step may take some minutes/hours dpending on the length of the corpus."
      ],
      "metadata": {
        "id": "dDdfrfZHu9b9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOGlte_SOe8T"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "input_folder=/content/xhosa/ready\n",
        "output_folder=/content/xhosa/aligned\n",
        "cd fairseq/\n",
        "for audio in $input_folder/*.wav; do\n",
        "  filename=\"$(basename \"$audio\")\"\n",
        "  stem=${filename%.*}\n",
        "  output_path=$output_folder/$stem\n",
        "  rm -rf $output_path\n",
        "  python -m examples.mms.data_prep.align_and_segment \\\n",
        "  --audio_filepath $input_folder/$stem.wav \\\n",
        "  --text_filepath $input_folder/$stem.txt \\\n",
        "  --lang xho \\\n",
        "  --outdir $output_path \\\n",
        "  --uroman /content/uroman/bin\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two following cells are optional. If you are not interested in the chunks or .json files, you can skip them and you will be able to download the TextGrids later."
      ],
      "metadata": {
        "id": "AVf-OQ2Fuh8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTIONAL : Run the following cell to save `aligned` folder containing the chunks and one manifest.json (time stamps) for each original file you have provided. into your Drive."
      ],
      "metadata": {
        "id": "L_aQuorx18cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/aligned.zip /content/xhosa/aligned"
      ],
      "metadata": {
        "id": "_Yfg-zweNFKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTIONAL : Run the following cell to DOWNLOAD `aligned` folder containing the chunks and one manifest.json (time stamps) for each original file you have provided."
      ],
      "metadata": {
        "id": "LlndFhjE1laR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r aligned.zip /content/xhosa/aligned\n",
        "from google.colab import files\n",
        "files.download('/content/aligned.zip')"
      ],
      "metadata": {
        "id": "8vJZb36V1Xyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting the aligned transcriptions to .TextGrid\n",
        "So you can open it in Praat or convert it to .eaf or any other compatible format."
      ],
      "metadata": {
        "id": "_FBwOGuk160G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO :replace py paths by my git repository"
      ],
      "metadata": {
        "id": "523rvkjR3EXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textgrid"
      ],
      "metadata": {
        "id": "VUZLfVDL_o2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "input_directory=\"/content/xhosa/aligned/\"\n",
        "output_directory=\"/content/xhosa/textgrids/\"  # All output files will be saved here\n",
        "\n",
        "# Ensure the output directory exists\n",
        "mkdir -p \"$output_directory\"\n",
        "\n",
        "# Find all 'manifest.json' files under the input_directory\n",
        "find \"$input_directory\" -type f -name \"manifest.json\" | while IFS= read -r manifest; do\n",
        "  # Extract the directory of the manifest and the name of the subdirectory containing the manifest\n",
        "  manifest_directory=$(dirname \"$manifest\")\n",
        "  subdirectory_name=$(basename \"$manifest_directory\")\n",
        "\n",
        "  # Construct the output file path using the subdirectory name for uniqueness\n",
        "  output_file_path=\"${output_directory}${subdirectory_name}.TextGrid\"\n",
        "\n",
        "  # Call the python script with the manifest and output file path\n",
        "  python /content/fa_xhosa/json_to_textgrid.py \"$manifest\" \"$output_file_path\"\n",
        "\n",
        "  echo \"Processed $manifest into $output_file_path\"\n",
        "done\n"
      ],
      "metadata": {
        "id": "RKhxWPlA3D5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the TextGrid files into your drive."
      ],
      "metadata": {
        "id": "PkPeUE2RTWqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/textgrids.zip /content/xhosa/textgrids"
      ],
      "metadata": {
        "id": "YE2qj5HGF_gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the TextGrid files into your computer."
      ],
      "metadata": {
        "id": "fBFKdZ9F26HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip textgrids.zip /content/xhosa/textgrids\n",
        "from google.colab import files\n",
        "files.download('/content/textgrids.zip')"
      ],
      "metadata": {
        "id": "ZR6urk7Q22vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges and future directions <a name=\"challenges\"></a>"
      ],
      "metadata": {
        "id": "bE2u0oiHlvXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robustness <a name=\"robust\"></a>"
      ],
      "metadata": {
        "id": "Ahtzfjq241E6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U4II65s_ly7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is inspired by [this](https://colab.research.google.com/github/cawoylel/nlp4all/blob/main/asr/src/asr_tutorial.ipynb) tutorial from [_NLP4ALL_](https://github.com/cawoylel/nlp4all) which is focused on simplifying the process of building NLP models for underrepresented languages and making it more accessible: \" We aim to provide a replicable framework that communities can adapt for their languages, aligning with our vision of making NLP technology widely accessible.\""
      ],
      "metadata": {
        "id": "7G5CpdCKs1JC"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}