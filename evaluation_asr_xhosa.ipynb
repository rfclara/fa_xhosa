{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuAgjCkip2lYw0bTxdV8bT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rfclara/fa_xhosa/blob/main/evaluation_asr_xhosa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Are evangelist speech models able to understand natural language ?"
      ],
      "metadata": {
        "id": "pyMFtkl8swgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we explore the performance of an existing Automatic Speech Recognition (ASR) model for Xhosa, developed under the [Massively Multilingual Speech (MMS)](https://huggingface.co/docs/transformers/model_doc/mms) project by Facebook AI Research. The model's evaluation is conducted using a natural data corpus provided by the University of Gothenburg's Spraakbanken (Language Bank), accessible at [Spraakbanken's Xhosa Corpus](https://spraakbanken.gu.se/korp/?mode=xhosa#?lang=eng&cqp=%5B%5D&corpus=xhosa).\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. **Align and preprocess the data:**\n",
        "   - The original Spraakbanken's Xhosa Corpus is fully manually transcribed by students in South Africa who have undergone specific training. Each sentence has been aligned to the corresponding audio using the MMS [forced alignment tool](https://github.com/facebookresearch/fairseq/tree/main/examples/mms/data_prep), as detailed in this [Notebook](https://github.com/rfclara/fa_xhosa/blob/main/xhosa_forced_alignement.ipynb). The preprocessing involved removing punctuation and comments such as \\<laugh> or \\<code-switching> to ensure the corpus is in a consistent format suitable for evaluation.\n",
        "\n",
        "2. **Evaluate Existing MMS ASR Model:**\n",
        "  - Assess the robustness and accuracy of the MMS ASR model for Xhosa using a natural language dataset from Spraakbanken. Thanks to this natural data, we will have the opportinity to verify Meta's claim: \"while the content of the audio recordings is religious, our analysis shows that this does not overly bias the model to produce more religious language‚Äù.\n",
        "\n",
        "\n",
        "3. **Fine-Tuning the ASR Model:**\n",
        "   - Investigate whether fine-tuning the MMS ASR model on the natural data corpus improves its performance, given that the original model appears to be primarily trained on biblical texts.\n",
        "\n",
        "4. **Compare the performances:**\n",
        "Compare WER and CER metrix for the MMS ASR model before and after fine-tunning.\n",
        "\n",
        "## Structure of the Notebook\n",
        "\n",
        "1. **Data Preparation:**\n",
        "   - Steps to download and preprocess the dataset.\n",
        "   \n",
        "2. **Model Evaluation:**\n",
        "   - Application of the MMS ASR model on the prepared dataset.\n",
        "   - Performance metrics and error analysis.\n",
        "   \n",
        "3. **Fine-Tuning:**\n",
        "   - Process of fine-tuning the MMS ASR model using the natural data corpus.\n",
        "   - Comparison of performance metrics before and after fine-tuning.\n",
        "   \n",
        "4. **Results and Discussion:**\n",
        "   - Insights gained from the evaluation.\n",
        "   - Discussion on the model's strengths and areas for improvement.\n",
        "   \n",
        "5. **Conclusion:**\n",
        "   - Summary of findings.\n",
        "   - Future work and potential improvements.\n",
        "\n",
        "By following this structured approach, we aim to provide a comprehensive assessment of the MMS ASR model's capabilities in handling real-world Xhosa speech data, contributing valuable insights to the ongoing development and refinement of multilingual ASR technologies.\n"
      ],
      "metadata": {
        "id": "zZ4uQ-IFfDBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Set Up the Environment\n",
        "Install required libraries and import the requiered packages"
      ],
      "metadata": {
        "id": "-sVFRaqPe4wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio transformers jiwer\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import torch\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "-DucTBgPHHuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/fairseq\n",
        "!pwd\n",
        "%cd \"/content/fairseq\"\n",
        "!pip install --editable ./\n",
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "Bo8kapnpIw7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load the Model\n",
        "Load the processor and model for Xhosa from Facebook's MMS"
      ],
      "metadata": {
        "id": "e0G_yPFqexW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
        "\n",
        "model_id = \"facebook/mms-1b-all\" # mms-1b-fl102, mms-1b-l1107, mms-1b-all\n",
        "target_lang = \"xho\"  # Xhosa language code\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id, target_lang=target_lang)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang, ignore_mismatched_sizes=True)"
      ],
      "metadata": {
        "id": "jKH-Fx-PHKv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Download the clean and aligned audio files and gold transcriptions"
      ],
      "metadata": {
        "id": "ApyBWwOjls6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/aligned.zip /content\n",
        "!unzip /content/aligned.zip -d /"
      ],
      "metadata": {
        "id": "R8pJPQ-XHQWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Prediction\n"
      ],
      "metadata": {
        "id": "lF8jMvgjkmoe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW3GMygSGldi"
      },
      "outputs": [],
      "source": [
        "def load_and_process_audio(file_path):\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    waveform = waveform.squeeze()  # Remove channel dimension if it's mono\n",
        "    if sample_rate != 16000:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
        "    return waveform\n",
        "\n",
        "# Step 4: Iterate through directories and process each manifest file\n",
        "base_path = \"/content/xhosa/aligned\"\n",
        "all_transcriptions = []\n",
        "all_references = []\n",
        "\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    for file in files:\n",
        "        if file == \"manifest.json\":\n",
        "            manifest_path = os.path.join(root, file)\n",
        "            with open(manifest_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    entry = json.loads(line.strip())\n",
        "                    audio_path = entry[\"audio_filepath\"]\n",
        "                    reference_text = entry[\"normalized_text\"]\n",
        "                    if not reference_text.strip():\n",
        "                      continue\n",
        "                    # Load and process the audio file\n",
        "                    waveform = load_and_process_audio(audio_path)\n",
        "\n",
        "                    # Process the audio input\n",
        "                    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "                    # Ensure input is 2D [batch_size, sequence_length]\n",
        "                    if inputs.input_values.dim() == 3 and inputs.input_values.size(1) == 2:\n",
        "                        inputs.input_values = inputs.input_values.mean(dim=1)  # Convert stereo to mono\n",
        "\n",
        "                    # Perform inference\n",
        "                    with torch.no_grad():\n",
        "                        logits = model(inputs.input_values).logits\n",
        "\n",
        "                    # Decode the predicted IDs to text\n",
        "                    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "                    transcription = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "                    # Collect transcriptions and references\n",
        "                    all_transcriptions.append(transcription)\n",
        "                    all_references.append(reference_text)\n",
        "\n",
        "                    # Print the result for each segment\n",
        "                    print(f\"Transcription: {transcription}\")\n",
        "                    print(f\"Reference: {reference_text}\")\n",
        "                    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Evaluation\n",
        "\n",
        "*   WER\n",
        "*   CER\n",
        "\n"
      ],
      "metadata": {
        "id": "sTj0Qne4kiAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "wsY-TlCF3RBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jiwer\n",
        "from jiwer import wer, cer\n",
        "wer = wer(all_references, all_transcriptions)\n",
        "cer = cer(all_references, all_transcriptions)\n",
        "print(\"Word Error Rate (WER):\", wer)\n",
        "print(f\"Character Error Rate (CER): {cer}\")\n",
        "out = jiwer.process_words(\n",
        "    all_references,\n",
        "    all_transcriptions,\n",
        ")\n",
        "\n",
        "print(jiwer.visualize_alignment(out))\n"
      ],
      "metadata": {
        "id": "3LH8tJ5fPok9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}